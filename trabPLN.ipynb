{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Delcarações e Funções"
      ],
      "metadata": {
        "id": "G5uTOcrJrsh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18HpxHYRl6Nl",
        "outputId": "c603e9b1-ad21-48b7-fe7c-49fa8a7714d8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-28 19:45:27.935911: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.4.0/pt_core_news_md-3.4.0-py3-none-any.whl (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-md==3.4.0) (3.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Python libraries\n",
        "import io\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "e8RGRQmu2cMv"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Util function to read a plain text file\n",
        "def read_text_file(file_path):\n",
        "    text = \"\"\n",
        "    with io.open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "NbXnRLzc41Sl"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return an array with the clean words of the document\n",
        "def get_doc_words(plain_text):\n",
        "    doc_words = []\n",
        "    \n",
        "    # Cleaing the text\n",
        "    clean_text = re.sub('[^-a-zA-ZÀ-ÖØ-öØ-ÿ]+|[.,!?;]', ' ', plain_text.lower())\n",
        "    \n",
        "    # Tokenize sentences in words\n",
        "    doc_words = clean_text.split()\n",
        "    \n",
        "    return doc_words"
      ],
      "metadata": {
        "id": "-kj78yez5CbA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get text sample\n",
        "file_path = \"corpus.txt\"\n",
        "plain_text = read_text_file(file_path)\n",
        "len(plain_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N22MMvb5Gy-",
        "outputId": "b61e86d9-5bc2-44d2-d0c3-5c01da31cd59"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5133765"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create document word list\n",
        "doc_words = get_doc_words(plain_text)\n",
        "len(doc_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxUp-A1n5Ssm",
        "outputId": "c390fe7c-f6be-454d-a224-46827eb7460e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "804504"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 100 first words of the vocabulary\n",
        "print(doc_words[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_41o2x9-5z9J",
        "outputId": "302cf75b-2620-42d3-d290-d939bd644cfa"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['curiosidades', 'nos', 'últimos', 'séculos', 'os', 'avanços', 'na', 'área', 'da', 'medicina', 'foram', 'impressionantes', 'o', 'que', 'há', 'muitos', 'anos', 'parecia', 'impossível', 'hoje', 'já', 'é', 'realidade', 'como', 'os', 'transplantes', 'de', 'variados', 'órgãos', 'e', 'tecidos', 'os', 'avanços', 'nessa', 'área', 'foram', 'tão', 'surpreendentes', 'que', 'hoje', 'é', 'possível', 'até', 'mesmo', 'realizar', 'um', 'transplante', 'de', 'fezes', 'apesar', 'de', 'pouco', 'comum', 'o', 'transplante', 'de', 'fezes', 'é', 'realizado', 'por', 'algumas', 'pessoas', 'e', 'é', 'uma', 'alternativa', 'para', 'o', 'tratamento', 'de', 'alguns', 'tipos', 'de', 'diarreias', 'persistentes', 'conheça', 'a', 'seguir', 'um', 'pouco', 'mais', 'sobre', 'esse', 'processo', 'nada', 'convencional', 'o', 'transplante', 'de', 'fezes', 'ou', 'transplante', 'de', 'microbiota', 'fecal', 'o', 'transplante', 'de', 'fezes', 'também']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create term frequency list of the document words\n",
        "term_freq = Counter(doc_words)\n",
        "term_freq['que']"
      ],
      "metadata": {
        "id": "lMIymi2B58qU",
        "outputId": "fcfb7520-4b6e-454d-e38e-91e962628283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18512"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Spell Checker class\n",
        "class SpellChecker:\n",
        "    \n",
        "    def __init__(self, term_freq):\n",
        "        \"Constructor.\"\n",
        "        self.w_rank = {}\n",
        "        self.letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        \n",
        "        N = sum(term_freq.values())\n",
        "        for term in term_freq:\n",
        "            self.w_rank[term] = term_freq[term] / N\n",
        "    \n",
        "    def P(self, word): \n",
        "        \"Probability of 'word'.\"\n",
        "        return self.w_rank.get(word, 0)\n",
        "\n",
        "    def known(self, words): \n",
        "        \"The subset of 'words' that appear in the dictionary of w_rank.\"\n",
        "        return set(w for w in words if w in self.w_rank)\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from 'word'.\"\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in self.letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in self.letters]\n",
        "        \n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word): \n",
        "        \"All edits that are two edits away from 'word'.\"\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "    \n",
        "    def correction(self, word):\n",
        "        \"Most probable spelling correction for word.\"\n",
        "        return max(self.candidates(word), key = self.P)\n",
        "    \n",
        "    def candidates(self, word): \n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])"
      ],
      "metadata": {
        "id": "6imNNVNc65Hc"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "sp_model = SpellChecker(term_freq)"
      ],
      "metadata": {
        "id": "B_AIlUJq6_uv"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get probability of the word 'the'\n",
        "sp_model.P('árvore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x11ENMSI7At-",
        "outputId": "62f2b1ca-cf05-4309-c64b-42f2a46a2183"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00010068315384385908"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get words that exist in the dictionary\n",
        "sp_model.known(['trabalhar', 'não-mencionado'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isyL54xV7ESd",
        "outputId": "ac5daddf-096d-4286-b460-f89c3e419e8d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'trabalhar'}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the correction for word 'castli'\n",
        "sp_model.correction('traalo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K2nappGY7pNs",
        "outputId": "33ce9dcc-09f3-460d-f685-47b2f9f54032"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trabalho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get candidates for word 'wlak'\n",
        "sp_model.candidates('arvore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJL_fO6A8GH0",
        "outputId": "8d168d06-d923-430f-d871-c6961fe8e273"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arborea', 'ardor'}"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Executáveis**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eZ97EhkvrYtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textoParaAnalise = 'qur abaçar um árvre?'"
      ],
      "metadata": {
        "id": "RCYN5O8znTx6"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recebe uma string com o texto a ser corrigido, e o corrige Ortográficamente.\n",
        "\n",
        "\n",
        "frase = get_doc_words(textoParaAnalise)\n",
        "fraseCorrigida = ''\n",
        "\n",
        "for palavra in frase:\n",
        "  fraseCorrigida += sp_model.correction(palavra) + ' '\n",
        "\n",
        "print(fraseCorrigida)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LgkqO5DVz9k",
        "outputId": "da7362ea-a0eb-4645-cb67-c46095aa8c7f"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "que abraçar um árvore \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checa se o plural das palavras e o gênero delas concordam.\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_md\")\n",
        "doc = nlp(fraseCorrigida)\n",
        "print('Frase: ' + doc.text)\n",
        "for token in doc:\n",
        "    if(token.morph.get('Gender') != []):\n",
        "      if(token.morph.get('Gender') != last.morph.get('Gender') and last.morph.get('Gender') != []):\n",
        "        print('-> ' + last.text +' <-'+ ' \\nEstá errado. Gênero da próxima palavra não concorda com -> '+ token.text +' <-.')\n",
        "    \n",
        "      if(token.morph.get('Number') != []):\n",
        "        if(token.morph.get('Number') != last.morph.get('Number') and last.morph.get('Number') != []):\n",
        "          print('-> ' + last.text +' <-'+ ' \\nEstá errado. Plural da próxima palavra não concorda com -> '+ token.text +' <-.')\n",
        "    last = token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbVil3TCix9I",
        "outputId": "21121386-4985-451b-add5-992dce5e4285"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: que abraçar um árvore \n",
            "-> um <- \n",
            "Está errado. Gênero da próxima palavra não concorda com -> árvore <-.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CP1WSXT9rCls"
      }
    }
  ]
}